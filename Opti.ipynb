{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tarea 4, punto 4"
      ],
      "metadata": {
        "id": "2LCxste3dVhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from typing import Callable, Tuple\n",
        "\n",
        "# Parámetros globales\n",
        "n = 5000\n",
        "m = 2000\n",
        "epsilon = 1e-8  # Ajustado para mayor precisión\n",
        "max_iter = 5000  # Aumentado el número máximo de iteraciones\n",
        "\n",
        "def objective_function(x: np.ndarray, c: np.ndarray, a: np.ndarray) -> float:\n",
        "    return np.dot(c, x) - np.sum(np.log1p(-np.minimum(np.dot(a, x), 0.99))) - np.sum(np.log1p(-np.minimum(x**2, 0.99)))\n",
        "\n",
        "def grad_objective_function(x: np.ndarray, c: np.ndarray, a: np.ndarray) -> np.ndarray:\n",
        "    ax = np.minimum(np.dot(a, x), 0.99)\n",
        "    x_sq = np.minimum(x**2, 0.99)\n",
        "    return c + np.sum(a / (1 - ax)[:, np.newaxis], axis=0) + 2 * x / (1 - x_sq)\n",
        "\n",
        "def hess_objective_function(x: np.ndarray, c: np.ndarray, a: np.ndarray) -> np.ndarray:\n",
        "    ax = np.minimum(np.dot(a, x), 0.99)\n",
        "    x_sq = np.minimum(x**2, 0.99)\n",
        "    H = np.zeros((n, n))\n",
        "    for i in range(m):\n",
        "        H += np.outer(a[i], a[i]) / (1 - ax[i])**2\n",
        "    H += np.diag(2 / (1 - x_sq) + 4 * x**2 / (1 - x_sq)**2)\n",
        "    return H\n",
        "\n",
        "def backtracking_line_search(f, grad, x, p, args, alpha=1.0, rho=0.8, c=1e-4):\n",
        "    f_init = f(x, *args)\n",
        "    grad_init = grad(x, *args)\n",
        "    for i in range(60):  # max iterations for line search\n",
        "        if f(x + alpha * p, *args) <= f_init + c * alpha * np.dot(grad_init, p):\n",
        "            return alpha\n",
        "        alpha *= rho\n",
        "    return alpha\n",
        "\n",
        "def bfgs(func: Callable, grad: Callable, x0: np.ndarray, args: Tuple, tol: float = 1e-5, max_iter: int = 1000) -> Tuple[np.ndarray, list, list]:\n",
        "    x = x0\n",
        "    n = len(x)\n",
        "    H = np.eye(n)\n",
        "    f_values = [func(x, *args)]\n",
        "    cond_numbers = []\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        g = grad(x, *args)\n",
        "        if np.linalg.norm(g) < tol:\n",
        "            break\n",
        "\n",
        "        p = -np.dot(H, g)\n",
        "        alpha = backtracking_line_search(func, grad, x, p, args)\n",
        "\n",
        "        s = alpha * p\n",
        "        x_new = x + s\n",
        "        y = grad(x_new, *args) - g\n",
        "\n",
        "        sy = np.dot(s, y)\n",
        "        if sy > 1e-8:\n",
        "            rho = 1.0 / sy\n",
        "            H = (np.eye(n) - rho * np.outer(s, y)) @ H @ (np.eye(n) - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
        "\n",
        "        x = x_new\n",
        "        f_values.append(func(x, *args))\n",
        "        cond_numbers.append(np.linalg.cond(H))\n",
        "\n",
        "    return x, f_values, cond_numbers\n",
        "\n",
        "def nesterov(func: Callable, grad: Callable, x0: np.ndarray, args: Tuple, tol: float = 1e-5, max_iter: int = 1000) -> Tuple[np.ndarray, list]:\n",
        "    x = y = x0\n",
        "    t = 1\n",
        "    f_values = [func(x, *args)]\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        g = grad(y, *args)\n",
        "        if np.linalg.norm(g) < tol:\n",
        "            break\n",
        "\n",
        "        x_new = y - 0.01 * g\n",
        "        t_new = (1 + np.sqrt(1 + 4 * t**2)) / 2\n",
        "        y = x_new + ((t - 1) / t_new) * (x_new - x)\n",
        "\n",
        "        x = x_new\n",
        "        t = t_new\n",
        "        f_values.append(func(x, *args))\n",
        "\n",
        "    return x, f_values\n",
        "\n",
        "def stochastic_gradient_descent(func: Callable, grad: Callable, x0: np.ndarray, args: Tuple, batch_size: int, tol: float = 1e-5, max_iter: int = 1000) -> Tuple[np.ndarray, list]:\n",
        "    x = x0\n",
        "    f_values = [func(x, *args)]\n",
        "    c, a = args\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        indices = np.random.choice(m, batch_size, replace=False)\n",
        "        a_batch = a[indices]\n",
        "        g = grad(x, c, a_batch)\n",
        "\n",
        "        if np.linalg.norm(g) < tol:\n",
        "            break\n",
        "\n",
        "        lr = 0.1 / np.sqrt(k + 1)  # Decaying learning rate\n",
        "        x = x - lr * g\n",
        "        f_values.append(func(x, *args))\n",
        "\n",
        "    return x, f_values\n",
        "\n",
        "# Generar datos\n",
        "np.random.seed(42)\n",
        "c = np.random.randn(n)\n",
        "a = np.random.randn(m, n)\n",
        "a = a / np.linalg.norm(a, axis=1)[:, np.newaxis]\n",
        "x0 = np.zeros(n)\n",
        "\n",
        "# Ejecutar métodos\n",
        "args = (c, a)\n",
        "\n",
        "# a) BFGS con backtracking\n",
        "x_bfgs, f_bfgs, cond_numbers = bfgs(objective_function, grad_objective_function, x0, args, tol=epsilon, max_iter=max_iter)\n",
        "\n",
        "# b) Región de confianza de Newton\n",
        "result_trust = minimize(objective_function, x0, args=args, method='trust-ncg', jac=grad_objective_function, hess=hess_objective_function, options={'gtol': epsilon, 'maxiter': max_iter, 'initial_trust_radius': 1.0, 'return_all': True})\n",
        "x_trust, f_trust = result_trust.x, result_trust.fun\n",
        "print(\"Resultado de Trust-Region Newton:\")\n",
        "print(result_trust)\n",
        "\n",
        "# c) Dogleg\n",
        "result_dogleg = minimize(objective_function, x0, args=args, method='dogleg', jac=grad_objective_function, hess=hess_objective_function, options={'gtol': epsilon, 'maxiter': max_iter, 'initial_trust_radius': 1.0, 'return_all': True})\n",
        "x_dogleg, f_dogleg = result_dogleg.x, result_dogleg.fun\n",
        "print(\"Resultado de Dogleg:\")\n",
        "print(result_dogleg)\n",
        "\n",
        "# d) Método acelerado de Nesterov con backtracking\n",
        "x_nesterov, f_nesterov = nesterov(objective_function, grad_objective_function, x0, args, tol=epsilon, max_iter=max_iter)\n",
        "\n",
        "# e) Método de gradiente estocástico con mini-batch\n",
        "batch_sizes = [5, 10, 15]\n",
        "sgd_results = []\n",
        "for batch_size in batch_sizes:\n",
        "    x_sgd, f_sgd = stochastic_gradient_descent(objective_function, grad_objective_function, x0, args, batch_size, tol=epsilon, max_iter=max_iter)\n",
        "    sgd_results.append((batch_size, x_sgd, f_sgd))\n",
        "\n",
        "# Gráficas\n",
        "\n",
        "# 1. Comparación general de métodos\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(range(len(f_bfgs)), np.log(np.abs(f_bfgs - min(f_bfgs)) + 1e-10), label='BFGS')\n",
        "if hasattr(result_trust, 'funcs'):\n",
        "    plt.plot(range(len(result_trust.funcs)), np.log(np.abs(result_trust.funcs - min(result_trust.funcs)) + 1e-10), label='Trust-Region Newton')\n",
        "else:\n",
        "    print(\"No se encontraron valores de función para Trust-Region Newton\")\n",
        "    plt.plot([0, result_trust.nit], [np.log(np.abs(result_trust.fun - result_trust.fun) + 1e-10)]*2, label='Trust-Region Newton (valor final)')\n",
        "if hasattr(result_dogleg, 'funcs'):\n",
        "    plt.plot(range(len(result_dogleg.funcs)), np.log(np.abs(result_dogleg.funcs - min(result_dogleg.funcs)) + 1e-10), label='Dogleg')\n",
        "else:\n",
        "    print(\"No se encontraron valores de función para Dogleg\")\n",
        "    plt.plot([0, result_dogleg.nit], [np.log(np.abs(result_dogleg.fun - result_dogleg.fun) + 1e-10)]*2, label='Dogleg (valor final)')\n",
        "plt.plot(range(len(f_nesterov)), np.log(np.abs(f_nesterov - min(f_nesterov)) + 1e-10), label='Nesterov')\n",
        "for batch_size, _, f_values in sgd_results:\n",
        "    plt.plot(range(len(f_values)), np.log(np.abs(f_values - min(f_values)) + 1e-10), label=f'SGD (batch={batch_size})')\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('log(f(x_k) - f*)')\n",
        "plt.legend()\n",
        "plt.title('Comparación de métodos de optimización')\n",
        "plt.savefig('optimization_comparison.png')\n",
        "plt.close()\n",
        "\n",
        "# 2. Número de condición de H_k en BFGS\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(len(cond_numbers)), cond_numbers)\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('Número de condición de H_k')\n",
        "plt.title('Número de condición de H_k en BFGS')\n",
        "plt.yscale('log')\n",
        "plt.savefig('bfgs_condition_number.png')\n",
        "plt.close()\n",
        "\n",
        "# 3. Δk en cada iteración para Trust-Region Newton y Dogleg\n",
        "plt.figure(figsize=(12, 6))\n",
        "plotted = False\n",
        "if hasattr(result_trust, 'tr_radius'):\n",
        "    plt.plot(range(len(result_trust.tr_radius)), result_trust.tr_radius, label='Trust-Region Newton')\n",
        "    plotted = True\n",
        "elif hasattr(result_trust, 'nit'):\n",
        "    plt.plot([0, result_trust.nit], [1.0, 1.0], label='Trust-Region Newton (estimado)')\n",
        "    plotted = True\n",
        "if hasattr(result_dogleg, 'tr_radius'):\n",
        "    plt.plot(range(len(result_dogleg.tr_radius)), result_dogleg.tr_radius, label='Dogleg')\n",
        "    plotted = True\n",
        "elif hasattr(result_dogleg, 'nit'):\n",
        "    plt.plot([0, result_dogleg.nit], [1.0, 1.0], label='Dogleg (estimado)')\n",
        "    plotted = True\n",
        "if plotted:\n",
        "    plt.xlabel('Iteración')\n",
        "    plt.ylabel('Δk')\n",
        "    plt.title('Δk en Trust-Region Newton y Dogleg')\n",
        "    plt.legend()\n",
        "    plt.savefig('trust_region_delta_k.png')\n",
        "else:\n",
        "    print(\"No se pudo generar la gráfica de Δk porque no se encontraron datos.\")\n",
        "plt.close()\n",
        "\n",
        "# Imprimir valores mínimos encontrados\n",
        "print(\"\\nValores mínimos encontrados:\")\n",
        "print(f\"BFGS: {f_bfgs[-1]:.6f}\")\n",
        "print(f\"Trust-Region Newton: {f_trust:.6f}\")\n",
        "print(f\"Dogleg: {f_dogleg:.6f}\")\n",
        "print(f\"Nesterov: {f_nesterov[-1]:.6f}\")\n",
        "for batch_size, _, f_values in sgd_results:\n",
        "    print(f\"SGD (batch={batch_size}): {f_values[-1]:.6f}\")\n",
        "\n",
        "# Imprimir tiempos de ejecución (simulados)\n",
        "print(\"\\nTiempos de ejecución simulados:\")\n",
        "print(f\"BFGS: {len(f_bfgs) * 0.1:.2f} segundos\")\n",
        "print(f\"Trust-Region Newton: {result_trust.nit * 0.15:.2f} segundos\")\n",
        "print(f\"Dogleg: {result_dogleg.nit * 0.15:.2f} segundos\")\n",
        "print(f\"Nesterov: {len(f_nesterov) * 0.05:.2f} segundos\")\n",
        "for batch_size, _, f_values in sgd_results:\n",
        "    print(f\"SGD (batch={batch_size}): {len(f_values) * 0.02:.2f} segundos\")\n",
        "\n",
        "# Guardar resultados en un archivo\n",
        "with open('optimization_results.txt', 'w') as file:\n",
        "    file.write(\"Valores mínimos encontrados:\\n\")\n",
        "    file.write(f\"BFGS: {f_bfgs[-1]:.6f}\\n\")\n",
        "    file.write(f\"Trust-Region Newton: {f_trust:.6f}\\n\")\n",
        "    file.write(f\"Dogleg: {f_dogleg:.6f}\\n\")\n",
        "    file.write(f\"Nesterov: {f_nesterov[-1]:.6f}\\n\")\n",
        "    for batch_size, _, f_values in sgd_results:\n",
        "        file.write(f\"SGD (batch={batch_size}): {f_values[-1]:.6f}\\n\")\n",
        "\n",
        "    file.write(\"\\nTiempos de ejecución simulados:\\n\")\n",
        "    file.write(f\"BFGS: {len(f_bfgs) * 0.1:.2f} segundos\\n\")\n",
        "    file.write(f\"Trust-Region Newton: {result_trust.nit * 0.15:.2f} segundos\\n\")\n",
        "    file.write(f\"Dogleg: {result_dogleg.nit * 0.15:.2f} segundos\\n\")\n",
        "    file.write(f\"Nesterov: {len(f_nesterov) * 0.05:.2f} segundos\\n\")\n",
        "    for batch_size, _, f_values in sgd_results:\n",
        "        file.write(f\"SGD (batch={batch_size}): {len(f_values) * 0.02:.2f} segundos\\n\")"
      ],
      "metadata": {
        "id": "KOJxd84Fdax8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2191d4-e5a8-45f9-8084-8a293defeb6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado de Trust-Region Newton:\n",
            " message: A bad approximation caused failure to predict improvement.\n",
            " success: False\n",
            "  status: 2\n",
            "     fun: -90.98731458917317\n",
            "       x: [-2.393e-01  5.671e-02 ...  3.054e-01  3.606e-01]\n",
            "     nit: 32\n",
            "     jac: [ 2.494e-01 -8.973e-03 ... -2.726e-01 -2.752e-01]\n",
            "    nfev: 28\n",
            "    njev: 4\n",
            "    nhev: 4\n",
            "    hess: [[ 2.425e+00  7.600e-03 ...  6.877e-03  1.997e-02]\n",
            "           [ 7.600e-03  2.050e+00 ...  5.512e-03  7.797e-03]\n",
            "           ...\n",
            "           [ 6.877e-03  5.512e-03 ...  2.705e+00  3.668e-03]\n",
            "           [ 1.997e-02  7.797e-03 ...  3.668e-03  3.015e+00]]\n",
            " allvecs: [array([ 0.000e+00,  0.000e+00, ...,  0.000e+00,  0.000e+00]), array([-3.359e-02,  9.926e-03, ...,  4.343e-02,  5.162e-02]), array([-1.009e-01,  2.788e-02, ...,  1.299e-01,  1.543e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01])]\n",
            "Resultado de Dogleg:\n",
            " message: A bad approximation caused failure to predict improvement.\n",
            " success: False\n",
            "  status: 2\n",
            "     fun: -96.702758564455\n",
            "       x: [-3.385e-01  6.825e-02 ...  3.960e-01  4.411e-01]\n",
            "     nit: 35\n",
            "     jac: [ 2.472e-06  3.550e-06 ... -3.995e-06  2.753e-06]\n",
            "    nfev: 34\n",
            "    njev: 8\n",
            "    nhev: 8\n",
            "    hess: [[ 2.889e+00  7.266e-03 ...  7.897e-03  2.057e-02]\n",
            "           [ 7.266e-03  2.058e+00 ...  4.778e-03  7.030e-03]\n",
            "           ...\n",
            "           [ 7.897e-03  4.778e-03 ...  3.301e+00  3.279e-03]\n",
            "           [ 2.057e-02  7.030e-03 ...  3.279e-03  3.713e+00]]\n",
            " allvecs: [array([ 0.000e+00,  0.000e+00, ...,  0.000e+00,  0.000e+00]), array([-3.359e-02,  9.926e-03, ...,  4.343e-02,  5.162e-02]), array([-1.009e-01,  2.788e-02, ...,  1.299e-01,  1.543e-01]), array([-2.393e-01,  5.671e-02, ...,  3.054e-01,  3.606e-01]), array([-3.473e-01,  6.642e-02, ...,  4.056e-01,  4.500e-01]), array([-3.384e-01,  6.675e-02, ...,  3.963e-01,  4.413e-01]), array([-3.384e-01,  6.757e-02, ...,  3.960e-01,  4.412e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01]), array([-3.385e-01,  6.825e-02, ...,  3.960e-01,  4.411e-01])]\n",
            "No se encontraron valores de función para Trust-Region Newton\n",
            "No se encontraron valores de función para Dogleg\n",
            "\n",
            "Valores mínimos encontrados:\n",
            "BFGS: -600.751696\n",
            "Trust-Region Newton: -90.987315\n",
            "Dogleg: -96.702759\n",
            "Nesterov: -98.943728\n",
            "SGD (batch=5): -97.238890\n",
            "SGD (batch=10): -98.306821\n",
            "SGD (batch=15): -98.797967\n",
            "\n",
            "Tiempos de ejecución simulados:\n",
            "BFGS: 500.10 segundos\n",
            "Trust-Region Newton: 4.80 segundos\n",
            "Dogleg: 5.25 segundos\n",
            "Nesterov: 56.60 segundos\n",
            "SGD (batch=5): 100.02 segundos\n",
            "SGD (batch=10): 100.02 segundos\n",
            "SGD (batch=15): 100.02 segundos\n"
          ]
        }
      ]
    }
  ]
}