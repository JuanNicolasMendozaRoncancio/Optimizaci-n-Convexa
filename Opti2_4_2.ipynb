{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cuarto punto:\n",
        "\n",
        "Primero defininmos la función y su gradiente:"
      ],
      "metadata": {
        "id": "EMo7t28GIUDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "\n",
        "# Definición de la función f(x) y su gradiente\n",
        "def f(x, c, a_list):\n",
        "    m, n = len(a_list), len(x)\n",
        "    dot_products = np.array([np.dot(a, x) for a in a_list])\n",
        "    log_term1 = np.sum(np.log(np.maximum(1e-10, 1 - dot_products)))\n",
        "    log_term2 = np.sum(np.log(np.maximum(1e-10, 1 - x**2)))\n",
        "    return np.dot(c, x) - log_term1 - log_term2\n",
        "\n",
        "def grad_f(x, c, a_list):\n",
        "    m, n = len(a_list), len(x)\n",
        "    grad = c.copy()\n",
        "    for a in a_list:\n",
        "        dot_product = np.dot(a, x)\n",
        "        grad += a / np.maximum(1e-10, 1 - dot_product)\n",
        "    grad += 2 * x / np.maximum(1e-10, 1 - x**2)\n",
        "    return grad"
      ],
      "metadata": {
        "id": "5IwRa0jRuiX9"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se generan los vectores c y a_{j}:"
      ],
      "metadata": {
        "id": "U1VjcMo4IeKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generación de vectores c y a_list\n",
        "def generate_vectors(m, n):\n",
        "    c = np.random.randn(n)\n",
        "    a_list = [np.random.randn(n) for _ in range(m)]\n",
        "    a_list = [a / np.linalg.norm(a) for a in a_list]  # Normalizar a_j\n",
        "    return c, a_list"
      ],
      "metadata": {
        "id": "9X5jO7AdumCq"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la estimación del tamaño de paso que nos asegura la convergencia usamos que este debe ser menor a 2/L donde L es la constante de Lipschitz del gradiente.\n",
        "Para encontrar L implementamos el siguiente algoritmo:"
      ],
      "metadata": {
        "id": "l1ci-V69xOB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "def estimate_lipschitz_constant(c, a_list, n_samples=1000):\n",
        "    n = len(c)\n",
        "    L = 0\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        # Generar dos puntos aleatorios en el dominio [-0.99, 0.99]\n",
        "        x = np.random.uniform(-0.99, 0.99, n)\n",
        "        y = np.random.uniform(-0.99, 0.99, n)\n",
        "\n",
        "        # Calcular los gradientes\n",
        "        grad_x = grad_f(x, c, a_list)\n",
        "        grad_y = grad_f(y, c, a_list)\n",
        "\n",
        "        # Calcular las normas de las diferencias\n",
        "        grad_diff_norm = norm(grad_x - grad_y)\n",
        "        point_diff_norm = norm(x - y)\n",
        "\n",
        "        # Actualizar la estimación de L\n",
        "        if point_diff_norm > 0:\n",
        "            L = max(L, grad_diff_norm / point_diff_norm)\n",
        "\n",
        "    return L\n",
        "\n",
        "# Ejemplo de uso\n",
        "n, m = 5000, 2000\n",
        "c, a_list = generate_vectors(m, n)\n",
        "\n",
        "L_estimate = estimate_lipschitz_constant(c, a_list)\n",
        "print(f\"Estimación de la constante de Lipschitz L: {L_estimate}\")\n",
        "alpha = 2/L_estimate - 0.2/L_estimate\n",
        "print(f\"alpha: {alpha}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jixCUZj0E9p",
        "outputId": "98be818f-b629-4539-bfab-f46f0fd2bf5a"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimación de la constante de Lipschitz L: 2534743561.87601\n",
            "alpha: 7.101310077567716e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restringimos x a un dominio adecuado:"
      ],
      "metadata": {
        "id": "Wvqt2pMYI1Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función auxiliar para restringir x\n",
        "def clip_x(x):\n",
        "    return np.clip(x, -0.99, 0.99)"
      ],
      "metadata": {
        "id": "WQJ-7oc3v1k5"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se encuentran los codigos de cada uno de los algoritmos, en el siguiente orden:\n",
        "\n",
        "* Gradiente con tamaño de paso constante\n",
        "* Gradiente con backtracking\n",
        "* Gradiente con backtracking y la condición de wolfe\n",
        "* Metodo de Newton"
      ],
      "metadata": {
        "id": "OPbOeTN8I7eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Método del gradiente con tamaño de paso constante\n",
        "def gradient_descent(x0, c, a_list, alpha, max_iter, epsilon):\n",
        "    x = clip_x(x0.copy())\n",
        "    f_values = [f(x, c, a_list)]\n",
        "    start_time = time()\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad = grad_f(x, c, a_list)\n",
        "        if np.linalg.norm(grad) < epsilon:\n",
        "            break\n",
        "        x = clip_x(x - alpha * grad)\n",
        "        f_values.append(f(x, c, a_list))\n",
        "\n",
        "    return x, f_values, time() - start_time"
      ],
      "metadata": {
        "id": "DYd_QrOTuq33"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Método del gradiente con backtracking\n",
        "def gradient_descent_backtracking(x0, c, a_list, alpha, beta, max_iter, epsilon):\n",
        "    x = clip_x(x0.copy())\n",
        "    f_values = [f(x, c, a_list)]\n",
        "    start_time = time()\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad = grad_f(x, c, a_list)\n",
        "        if np.linalg.norm(grad) < epsilon:\n",
        "            break\n",
        "\n",
        "        t = 1.0\n",
        "        while f(clip_x(x - t * grad), c, a_list) > f(x, c, a_list) - alpha * t * np.dot(grad, grad):\n",
        "            t *= beta\n",
        "\n",
        "        x = clip_x(x - t * grad)\n",
        "        f_values.append(f(x, c, a_list))\n",
        "\n",
        "    return x, f_values, time() - start_time\n"
      ],
      "metadata": {
        "id": "rQo5MUi9uzAt"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Método del gradiente con backtracking y condición de Wolfe\n",
        "def gradient_descent_wolfe(x0, c, a_list, alpha, beta, c1, c2, max_iter, epsilon):\n",
        "    x = clip_x(x0.copy())\n",
        "    f_values = [f(x, c, a_list)]\n",
        "    start_time = time()\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad = grad_f(x, c, a_list)\n",
        "        if np.linalg.norm(grad) < epsilon:\n",
        "            break\n",
        "\n",
        "        t = 1.0\n",
        "        while True:\n",
        "            new_x = clip_x(x - t * grad)\n",
        "            if f(new_x, c, a_list) <= f(x, c, a_list) - c1 * t * np.dot(grad, grad):\n",
        "                new_grad = grad_f(new_x, c, a_list)\n",
        "                if np.dot(new_grad, -grad) >= c2 * np.dot(grad, -grad):\n",
        "                    break\n",
        "            t *= beta\n",
        "\n",
        "        x = new_x\n",
        "        f_values.append(f(x, c, a_list))\n",
        "\n",
        "    return x, f_values, time() - start_time\n"
      ],
      "metadata": {
        "id": "m2ttUz7bu3jC"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Método de Newton\n",
        "def newton_method(x0, c, a_list, max_iter, epsilon):\n",
        "    x = clip_x(x0.copy())\n",
        "    f_values = [f(x, c, a_list)]\n",
        "    start_time = time()\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad = grad_f(x, c, a_list)\n",
        "        if np.linalg.norm(grad) < epsilon:\n",
        "            break\n",
        "\n",
        "        # Cálculo de la matriz Hessiana (aproximación numérica)\n",
        "        h = 1e-8\n",
        "        hessian = np.zeros((len(x), len(x)))\n",
        "        for j in range(len(x)):\n",
        "            x_plus = x.copy()\n",
        "            x_plus[j] += h\n",
        "            grad_plus = grad_f(x_plus, c, a_list)\n",
        "            hessian[:, j] = (grad_plus - grad) / h\n",
        "\n",
        "        # Resolver el sistema lineal para obtener la dirección de Newton\n",
        "        direction = np.linalg.solve(hessian, -grad)\n",
        "\n",
        "        # Actualizar x\n",
        "        x = clip_x(x + direction)\n",
        "        f_values.append(f(x, c, a_list))\n",
        "\n",
        "    return x, f_values, time() - start_time"
      ],
      "metadata": {
        "id": "NckO3l4_u7In"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializamos los métodos con los siguientes parametros:\n",
        "\n",
        "* (alpha, betha) son (0.5,0.8), ( 0.8,0.3), (0.5,0.5) para el algoritmo de gradiente desendiente con backtracking\n",
        "* (alpha, betha, c1,c2) = (0.5, 0.8, 0.1, 0.9), (0.8, 0.3, 0.05, 0.5) y (0.5, 0.5, 0.07, 0.5) para gradiente descendiente con la condición de wolfe.\n"
      ],
      "metadata": {
        "id": "kHz3_rKJJn7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del problema\n",
        "n, m = 5000, 2000\n",
        "c, a_list = generate_vectors(m, n)\n",
        "x0 = np.zeros(n)\n",
        "max_iter = 1000\n",
        "epsilon = 1e-6\n",
        "\n",
        "# Ejecutar los métodos\n",
        "methods = {\n",
        "    \"Gradiente (paso constante)\": lambda: gradient_descent(x0, c, a_list, alpha, max_iter, epsilon),\n",
        "\n",
        "    \"Gradiente (backtracking) 1\": lambda: gradient_descent_backtracking(x0, c, a_list, 0.5, 0.8, max_iter, epsilon),\n",
        "    \"Gradiente (backtracking) 2\": lambda: gradient_descent_backtracking(x0, c, a_list, 0.8, 0.3, max_iter, epsilon),\n",
        "    \"Gradiente (backtracking) 3\": lambda: gradient_descent_backtracking(x0, c, a_list, 0.5, 0.5, max_iter, epsilon),\n",
        "\n",
        "    \"Gradiente (Wolfe) 1\": lambda: gradient_descent_wolfe(x0, c, a_list, 0.5, 0.8, 0.1, 0.9, max_iter, epsilon),\n",
        "    \"Gradiente (Wolfe) 2\": lambda: gradient_descent_wolfe(x0, c, a_list, 0.8, 0.3, 0.05, 0.5, max_iter, epsilon),\n",
        "    \"Gradiente (Wolfe) 3\": lambda: gradient_descent_wolfe(x0, c, a_list, 0.5, 0.5, 0.07, 0.5, max_iter, epsilon),\n",
        "\n",
        "    \"Newton\": lambda: newton_method(x0, c, a_list, max_iter, epsilon)\n",
        "}\n",
        "\n",
        "results = {}"
      ],
      "metadata": {
        "id": "PR2i-OGavBkV"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method_name, method_func in methods.items():\n",
        "    x_opt, f_values, execution_time = method_func()\n",
        "    f_star = min(f_values)  # Estimación del mínimo para este método\n",
        "    results[method_name] = (x_opt, f_values, execution_time, f_star)\n",
        "\n",
        "# Graficar resultados\n",
        "plt.figure(figsize=(12, 8))\n",
        "for method, (x_opt, f_values, time, f_star) in results.items():\n",
        "    plt.semilogy(range(len(f_values)), [abs(fv - f_star) for fv in f_values], label=f\"{method} ({time:.2f}s)\")\n",
        "\n",
        "plt.xlabel(\"Iteraciones\")\n",
        "plt.ylabel(\"log(|f(x_k) - f*|)\")\n",
        "plt.title(\"Comparación de métodos de optimización\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Resultados por método:\")\n",
        "for method, (x_opt, f_values, time, f_star) in results.items():\n",
        "    print(f\"{method}:\")\n",
        "    print(f\"  Tiempo de ejecución: {time:.2f} segundos\")\n",
        "    print(f\"  Valor final de f: {f_values[-1]:.6f}\")\n",
        "    print(f\"  Estimación de f*: {f_star:.6f}\")\n",
        "    print(f\"  Número de iteraciones: {len(f_values)}\")\n",
        "    print()\n",
        "\n",
        "# Encontrar el mejor f* global\n",
        "global_f_star = min(result[3] for result in results.values())\n",
        "print(f\"Mejor estimación global de f*: {global_f_star:.6f}\")"
      ],
      "metadata": {
        "id": "KJnWq4XDvJVU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}